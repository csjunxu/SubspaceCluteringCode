
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[numbers,sort]{natbib}
%\usepackage[UTF8]{ctex}


\usepackage{subfigure}
\usepackage{upgreek}
\usepackage{multirow}
\usepackage{color}
\usepackage{bm}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{arydshln}
\usepackage{latexsym}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conj}[theorem]{Conjecture}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}




\begin{document}

%%%%%%%%% TITLE
\title{Non-negative Least Squares Regression (Collaborative Representation) for Subspace Clustering}

\maketitle



%%%%%%%%% ABSTRACT
\begin{abstract}

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

The sparse subspace clustering (SSC) \cite{ssc} and low rank representation (LRR) \cite{lrr} methods construct the affinities matrix of an undirected graphs by sparse or low rank representations. However, the coefficients in these graphs can be negative, which allows the data points to ``cancel each other out'' by subtraction or include outliers which share opposite physical meaning, which is physically absurd for many visual analysis. For example, given a set of data points $\{\bm{x}_{1},...,\bm{x}_{n}\}$, if two of these points are just opposite to each other, i.e., $\bm{x}_{i}=-\bm{x}_{j}$. Then the representation of $\bm{x}_{i}$ over this set of data points would be $\bm{c}_{i}$, in which $\bm{c}_{ij}=-1$ while other coefficients would be zero. This is clearly not what we want since the two points $\bm{x}_{i}$ and $\bm{x}_{j}$ are just opposite and would be problematic to represent each other. It is better to give a detailed example or give a figure to illustrate this point. The other methods also employ \textsl{self-expressiveness} in which each data point can be represented by a linear combination of other data points that generally involve complex cancellations between positive and negative samples. This combination form lacks intuitive meaning in physical sense. However, the non-negative LSR only allows non-negative combination of multiple data points to additively represent the given data point, which is compatible with the intuitive notion of combining parts into a whole.

\subsection{Physically correct! Not only mathematically correct!}

Each related data points are activated to generate a new data point additively. From this point, the model can be viewed as a generative model. Besides,since we add a contraint of ``sum to one'', hence the overall amount to be added is limited and the model will automatically select the most similar points from the same subspace. From this point, the model also enjoys discriminative ability. Hence, with non-negative and sum-to-one constraints, the NNLSR model simultaneously has generative and discriminative properties. Seen in this light, the proposed model can be guaranteed to perform well on representation based classification and clustering problems. Considering that the generative and discriminative property often need ``sparse plus low rank'' prior modeling, which is solved by ADMM algorithm with three variables \cite{nnlrs}, the proposed model is very simple and can be solved under the ADMM framework with only two variables, in which the convergency condition can be guaranteed. 


In fact, non-negativity is more reasonable with the biological modeling of the visual data and often lead to better performance for data representation \cite{} and graph construction \cite{}. In many real-world problems, the underlying parameters which represent uantities can only take on non-negative values. Examples in this inlcude amounts of materials, chemical concentrations, pixel intensities, the compounds of endmenbers in hyperspectral images, to name a few. Our work is inherently benefit from the advantages of non-negative matrix factorization for parts-based facial analysis \cite{nmfnature}
 
Non-negatie least squares has been studied in \cite{slawski2013non} for sparse recovery without regularization. The authors compare with the non-negative LASSO method \cite{kim2007interior} and found that the proposed non-negative least squares model can achieve similar or even better performance on sparse recvoery problems. For $\bm{A}=(\bm{a}_{ij})$ we write $\bm{A}\ge0$ if $\bm{a}_{ij}\ge0$ for each $i$ and $j$ and we say $\bm{A}$ is a non-negative matrix. This notation can be naturally extended for vectors. Similarly we can define non-positive matrix, negative matrix, and negatvie matrix. The famous Perron-Frobenious theory are widely used to analysis non-negative matrices.

The proposed model can be solved under the framework of ADMM \cite{admm}. To make the method more efficient and scalable, we can also employ the linearized ADMM with adaptive penalty (LADMAP) \cite{ladmap}, which uses less auxiliary variables and no matrix inversion.


\section{Motivation}

\begin{itemize}
\item Positive collaborative representation could achieve sparse representation since similar points are sparse while dissimilar points are dense.

\item Positive supports are positive to self-reppresentation while negative supports are negative to self-representation.

\item Due to the space of constraint is only half of the original LSR model, the searching for solution does not need too many iterations such as in SSC and LRR method, hence the proposed method keeps the efficiency of LSR and the speed of our method is faster than the others.

\item In summmary, Physically feasible, higher accuray, and faster speed.
\end{itemize}

\section{LSR Model}
It is widely known that the least squares regression (LSR) model \cite{prml}, which can be expressed as follows:
\begin{equation}
\label{e1}
\min_{\bm{A}}
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{2}
+
\lambda
\|
\bm{A}
\|_{F}^{2}
,
\end{equation}
has a form of closed solution as
\begin{equation}
\label{e2}
\hat{\bm{A}}
=
(\bm{X}^{\top}\bm{X}+\lambda\bm{I})^{-1}\bm{X}^{\top}\bm{X}.
\end{equation}


In subspace clustering community, the LSR method \cite{lsr} proposed by Lu et al. can be formulated as follows:
\begin{equation}
\label{e3}
\min_{\bm{A}}
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{2}
+
\lambda
\|
\bm{A}
\|_{F}^{2}
\ 
\text{s.t.}
\ 
\text{diag}(\bm{A})=\bm{0}.
\end{equation}
Here we denote by $\text{diag}(\bm{A})$ both a diagonal matrix whose diagonal elements are the diagonal entries of $\bm{A}$ and the vector consisted of the diagonal elements. According to \cite{lsr}, the above problem has the optimal solution as 
\begin{equation}
\label{e4}
\hat{\bm{A}}
=
-\bm{\bm{Z}}(\text{diag}(\bm{Z}))
\
\text{s.t.} 
\
\text{diag}(\hat{\bm{A}})=\bm{0}
,
\end{equation}
where $\bm{Z}=(\bm{X}^{\top}\bm{X}+\lambda\bm{I})^{-1}$.

The constraint of  $\text{diag}(\bm{A})=\bm{0}$ in (\ref{e1}) could be removed and the LSR model achieves similar performance.


\section{Least Squares Regression with Constraint $\text{diag}(\bm{A})=\bm{0}$}
The LSR model can be reformulated as a collaborative representation model \cite{crc} for subspace clustering with an additional constraint of $\text{diag}(\bm{A})=\bm{0}$. The constraint of $\text{diag}(\bm{A})=\bm{0}$ is used to avoid the samples to represent themselves.

By introducing auxiliary variables into the optimization program, we can set 
$
\bm{C}
=
\bm{A}
$.
The LSR model (\ref{e1}) can be transformed into
\begin{equation}
\begin{split}
\label{e5}
&
\min_{\bm{A},\bm{C}}
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{F}
+
\lambda
\|
\bm{C}
\|_{F}^{2}
\\
& 
\text{s.t.}
\ 
\bm{C}=\bm{A}-\text{diag}(\bm{A})
,
\end{split}
\end{equation}
whose solution for $\bm{A}$ coincides with the solution of Eq. (\ref{e1}). By introducing a Lagrangian multipliers $\bm{\Delta}$ and a penalty parameter $\rho$, the Lagrangian function of the Eq. (\ref{e3}) can be written as
\begin{equation}
\begin{split}
\label{e6}
&
\mathcal{L}
(\bm{A},\bm{C},\bm{\Delta},\rho)
=
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{F}
+
\lambda
\|
\bm{C}
\|_{F}^{2}
\\
&
+
\langle
\bm{\Delta},
\bm{C}-(\bm{A}-\text{diag}(\bm{A}))
\rangle
+
\frac{\rho}{2}
\|
\bm{C}-(\bm{A}-\text{diag}(\bm{A}))
\|_{F}^{2}
\end{split}
\end{equation}
Denote by ($\bm{C}_{k},\bm{A}_{k}$) the optimization variables at iteration $k$, by $\bm{\Delta}_{k}$ the Lagrangian multipliers at iteration $k$, and by $\rho$ the penalty parameter at iteration $k$. Taking detivatives of $\mathcal{L}$ with respect to the variables and setting the derivatives to be zeros, we can alternatively update the variables as follows:

(1) Obtain $\bm{A}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{A}$, while fixing ($\bm{C}_{k},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e7}
&
\bm{A}_{k+1}
=
\bm{J}-\text{diag}(\bm{J}),
\\
&
\bm{J}
=
(\bm{X}^{\top}\bm{X}+\frac{\rho}{2}\bm{I})^{-1}
(\bm{X}^{\top}\bm{X}+\frac{\rho}{2}\bm{C}_{k}+\frac{1}{2}\bm{\Delta}_{k})
\end{split}
\end{equation}

(2) Obtain $\bm{C}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{C}$, while fixing ($\bm{A}_{k+1},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e8}
&
\bm{C}_{k+1}
=
\arg\min_{\bm{C}}
\frac{\rho}{2}
\|
\bm{C}-(\bm{A}_{k+1}-\rho^{-1}\bm{\Delta}_{k})
\|_{F}^{2}
+
\lambda
\|
\bm{C}
\|_{F}^{2}
\end{split}
\end{equation}
This is a least squares regression problem which has a closed-form solution as 
\begin{equation}
\begin{split}
\label{e9}
\bm{C}_{k+1} 
=
(\rho+2\lambda)^{-1}(\rho\bm{A}_{k+1}-\bm{\Delta}_{k}).
\end{split}
\end{equation}

(3) Obtain the Lagrangian multipliers $\bm{\Delta}_{k+1}$ while fixing ($\bm{C}_{k+1},\bm{A}_{k+1}$):
\begin{equation}
\begin{split}
\label{e10}
\bm{\Delta}_{k+1}
&
=
\bm{\Delta}_{k}
+
\rho
(\bm{C}_{k+1}-\bm{A}_{k+1})
.
\end{split}
\end{equation}

\textbf{Convergency analysis?}

\section{Non-Negative Least Squares Regression with $\text{diag}(\bm{A})=\bm{0}$}

This model enforces non-negative representation and hence produce sparse solutions, in the sense that it results only a few non-negative coefficients.

The performance of this method is much better than the original least squares regression (LSR) based subspace clustering method proposed by Lu et al. \cite{lsr}.

The LSR model in \cite{lsr} can be reformulated as a collaborative representation model \cite{crc} for subspace clustering with an additional constraint of $\text{diag}(\bm{A})=\bm{0}$. In this section, we want to mention that the coefficient matrix $\bm{C}$ with additional constraint could benefit the performance of subspace clustering. Motivated by the non-negative coefficient should share positive relationship while negative coefficients share negative relationship, we argue that non-negative representational coefficients should better represent the data points from the same subspace, while negative coefficients correspond to points from different subspaces. By this way, the negative coefficients will negatively influence the relationship among the points in the same subsapce and hence degrade the performance of the model on subspace clustering. Based on these observations, in this section, we propose to add an constraint on the coefficient matrix $\bm{A}$ that the elements in $\bm{A}$ should be non-negative, i.e., $\bm{A}\ge0$. Hence, the proposed non-negative collaborative representation model can be formulated as follows: 
\begin{equation}
\begin{split}
\label{e11}
&
\min_{\bm{A}}
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{2}
+
\lambda
\|
\bm{A}
\|_{F}^{2}
\\
\quad 
\text{s.t.}
\quad
&
\text{diag}(\bm{A})=\bm{0}
,
\bm{A}\ge0
,
\end{split}
\end{equation}
where $\bm{A}\ge0$ means that each element of $\bm{A}$ is non-negative.

By introducing auxiliary variables into the optimization program, we can set 
$
\bm{C}
=
\bm{A}
$.
The LSR model (\ref{e1}) can be transformed into
\begin{equation}
\begin{split}
\label{e12}
&
\min_{\bm{A},\bm{C}}
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{F}
+
\lambda
\|
\bm{C}
\|_{F}^{2}
\\
& 
\text{s.t.}
\ 
\bm{C}=\bm{A}-\text{diag}(\bm{A})
,
\bm{C}\ge0
,
\end{split}
\end{equation}
whose solution for $\bm{A}$ coincides with the solution of Eq. (\ref{e9}). By introducing a Lagrangian multipliers $\bm{\Delta}$ and a penalty parameter $\rho$, the Lagrangian function of the Eq. (\ref{e3}) can be written as
\begin{equation}
\begin{split}
\label{e13}
&
\mathcal{L}
(\bm{A},\bm{C},\bm{\Delta},\rho)
=
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{F}
+
\lambda
\|
\bm{C}
\|_{F}^{2}
\\
&
+
\langle
\bm{\Delta},
\bm{C}-(\bm{A}-\text{diag}(\bm{A}))
\rangle
+
\frac{\rho}{2}
\|
\bm{C}-(\bm{A}-\text{diag}(\bm{A}))
\|_{F}^{2}
\end{split}
\end{equation}

In this case, the matrix containing only zeros is a feasible starting point as it contains no negative values. After initializing the $\bm{A},\bm{C},\bm{\Delta}$ as zero matrices, the ADMM algorithm iterates consist of 1) minimizing $\mathcal{L}$ with respect to $\bm{A}$ while fixing the other variables, and 2) minimizing $\mathcal{L}$ with respect to $\bm{C}$ subject to the constraint $\bm{C}\ge0$ while fixing the other variables; 3) updating the Lagrangian variable $\bm{\Delta}$ while fixing the other variables. Specifically, denote by ($\bm{C}_{k},\bm{A}_{k}$) the optimization variables at iteration $k$, by $\bm{\Delta}_{k}$ the Lagrangian multipliers at iteration $k$. Taking detivatives of $\mathcal{L}$ with respect to the variables and setting the derivatives to be zeros, we can alternatively update the variables as follows:

(1) Obtain $\bm{A}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{A}$, while fixing ($\bm{C}_{k},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e14}
&
\bm{A}_{k+1}
=
\bm{J}-\text{diag}(\bm{J}),
\\
&
\bm{J}
=
(\bm{X}^{\top}\bm{X}+\frac{\rho}{2}\bm{I})^{-1}
(\bm{X}^{\top}\bm{X}+\frac{\rho}{2}\bm{C}_{k}+\frac{1}{2}\bm{\Delta}_{k})
\end{split}
\end{equation}

(2) Obtain $\bm{C}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{C}$, while fixing ($\bm{A}_{k+1},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e15}
\bm{C}_{k+1}
=
&
\arg\min_{\bm{C}}
\|
\bm{C}-(2\lambda+\rho)^{-1}(\rho\bm{A}_{k+1}-\bm{\Delta}_{k})
\|_{F}^{2}
\\
&
\quad
\text{s.t.}
\quad 
\bm{C}\ge 0
.
\end{split}
\end{equation}
This is a non-negative least squares problem which can be solved by many solves developed via active set method \cite{Nocedal2006NO} or specifically, the algorithm of Lawson and Hanson \cite{}.

(3) Obtain the Lagrangian multipliers $\bm{\Delta}_{k+1}$ while fixing ($\bm{C}_{k+1},\bm{A}_{k+1}$):
\begin{equation}
\begin{split}
\label{e16}
\bm{\Delta}_{k+1}
&
=
\bm{\Delta}_{k}
+
\rho
(\bm{C}_{k+1}-\bm{A}_{k+1})
.
\end{split}
\end{equation}

\textbf{Convergency analysis?}



\textbf{The above solution is slow when the number of column of $\bm{X}$ is much larger than its number of rows, i.e., when $N>d$. where $d$ is the dimension of features for each sample and $N$ is the number of samples in $\bm{X}$.} Hence, we propose the following solution via \textsl{Woodbury Identity} to reduce the computational cost for the inversion of the solution in Eq. (\ref{e12}).


\section{Large Scale Subset Selection Via Woodbury Identity}

The Woodbury Identity is 
\begin{equation}
(
\bm{A}
+
\bm{U}
\bm{C}
\bm{V}
)^{-1}
=
\bm{A}^{-1}
-
\bm{A}^{-1}
\bm{U}
(
\bm{C}^{-1}
+
\bm{V}\bm{A}^{-1}\bm{U}
)^{-1}
\bm{V}\bm{A}^{-1}
.
\end{equation}

Then the first step of updating $\bm{A}$ can be formulated as follows;

(1) Obtain $\bm{A}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{A}$, while fixing ($\bm{C}_{k},\bm{\Delta}_{k}$). This is equalivalently to solve the following problem
\begin{equation}
\begin{split}
\label{e18}
&
\bm{A}_{k+1}
=
\bm{J}-\text{diag}(\bm{J}),
\\
&
\bm{J}
=
(\bm{X}^{\top}\bm{X}+\frac{\rho}{2}\bm{I})^{-1}
(\bm{X}^{\top}\bm{X}
+
\frac{\rho}{2}
\bm{C}_{k}
+
\frac{1}{2}
\bm{\Delta}_{k}
)
\end{split}
\end{equation}
Since the matrices $\bm{X}^{\top}\bm{X}$ is of $N\times N$ dimension. It is computational expensive when $N$ is very large. By employing the Woodburry Identity mentioned above, we can have 
\begin{equation}
\label{e19}
(
\frac{\rho}{2}\bm{I}
+
\bm{X}^{\top}\bm{X}
)^{-1}
=
\frac{2}{\rho}\bm{I}
-
(\frac{2}{\rho})^{2}
\bm{X}^{\top}
(
\bm{I}
+
\frac{2}{\rho}\bm{X}\bm{X}^{\top}
)^{-1}
\bm{X}
.
\end{equation}
and transform this problem as
\begin{equation}
\begin{split}
\label{e20}
\bm{J}
=
&
(
\frac{2}{\rho}\bm{I}
-
(\frac{2}{\rho})^{2}
\bm{X}^{\top}
(
\bm{I}
+
\frac{2}{\rho}\bm{X}\bm{X}^{\top}
)^{-1}
\bm{X}
)
\\
&
*
(\bm{X}^{\top}\bm{X}
+
\frac{\rho}{2}
\bm{C}_{k}
+
\frac{1}{2}
\bm{\Delta}_{k}
)
\end{split}
\end{equation}
which will save a lot of computational costs.

The other parts are just the same as the previous section.


\section{Removing the Constraint of $\text{diag}(\bm{A})=\bm{0}$}

We can eliminate the constraint of $\text{diag}(\bm{A})=\bm{0}$. And the solution for each subproblem is obtained by removing the term of $\bm{A}=\bm{J}-\text{diag}(\bm{J})$. The physical meaning is that we can allow each data point to represente itself, so there always exists reasonable solutions even with outliers or when the data is insufficient. 

\section{Robust Large Scale Subset Selection via Dissimilarity based Outlier Detection?}


We can also introduce a dissimiarlity based matrix $\bm{D}$ to replace the $\ell_{p}$ or $\ell_{2,1}$ norms to ensure robustness. This can also remore the additional term $\bm{Z}$ on modeling the outliers with the restriction of $\ell_{1}$ norm. The matrix $\bm{D}$ should better be diagonal matrix. How to design the matrix $\bm{D}$ is another problem need to be solved.

Then the proposed model can be formulated as  
\begin{equation}
\label{e2}
\min_{\bm{A}}
\|
(\bm{X}
-
\bm{X}\bm{A})
\bm{D}
\|_{F}^{2}
+
\lambda
\|
\bm{A}
\|_{p,1}
.
\end{equation}
By introducing an auxiliary variable $\bm{C}$ into the optimization program, we can get
\begin{equation}
\begin{split}
\label{e3}
&
\min_{\bm{A},\bm{C}}
\|
(\bm{X}
-
\bm{X}\bm{A})
\bm{D}
\|_{F}^{2}
+
\lambda
\|
\bm{C}
\|_{p,1}
\ 
\text{s.t.}
\ 
\bm{C}=\bm{A}
.
\end{split}
\end{equation}
By introducing a Lagrangian multiplier $\bm{\Delta}$, the Lagrangian function of the Eq. (\ref{e3}) can be written as
\begin{equation}
\begin{split}
\label{e5}
&
\mathcal{L}
(\bm{A},\bm{C},\bm{\Delta},\rho)
=
\|
(\bm{X}
-
\bm{X}\bm{A})
\bm{D}
\|_{F}^{2}
+
\lambda
\|
\bm{C}
\|_{p,1}
\\
&
+
\langle
\bm{\Delta},
\bm{C}-\bm{A}
\rangle
+
\frac{\rho}{2}
\|
\bm{C}
-
\bm{A}
\|_{F}^{2}
\end{split}
\end{equation}
Denote by ($\bm{A}_{k},\bm{C}_{k}$) the optimization variables at iteration $k$, by $\bm{\Delta}_{k}$ the Lagrangian multiplier at iteration $k$, and by $\rho$ the penalty parameter at iteration $k$. Taking detivatives of $\mathcal{L}$ with respect to the variables and setting the derivatives to be zeros, we can alternatively update the variables as follows:

(1) Obtain $\bm{A}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{A}$, while fixing ($\bm{C}_{k},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e8}
&
\min_{\bm{A}}
\|
(\bm{X}
-
\bm{X}\bm{A})
\bm{D}
\|_{F}^{2}
+
\frac{\rho}{2}
\|
\bm{A}
-(
\bm{C}_{k}
-
\rho^{-1}
\bm{\Delta}_{k}
)
\|_{F}^{2},
\end{split}
\end{equation}
which is equalivalently to solve the following problem
\begin{equation}
\begin{split}
\label{e8}
\bm{X}^{\top}\bm{X}\bm{A}\bm{D}\bm{D}^{\top}
+
\frac{\rho}{2}
\bm{A}
=
\bm{X}^{\top}\bm{X}\bm{D}\bm{D}^{\top}
+
\frac{\rho}{2}
(
\bm{C}_{k}
-
\rho^{-1}
\bm{\Delta}_{k}
)
\end{split}
\end{equation}
Since the matrices $\bm{X}^{\top}\bm{X}$ and $\bm{D}^{\top}\bm{D}$ are positive semi-definite and positive definite, respectively. The above equation is a standard Sylvester equation which has a unique solution.


(2) Obtain $\bm{C}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{C}$, while fixing ($\bm{A}_{k+1},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e6}
\min_{\bm{C}}
\frac{1}{2}
\|
(\bm{A}_{k+1}+\rho^{-1}\bm{\Delta}{k})
-
\bm{C}
\|_{F}^{2}
+
\frac{\lambda}{\rho}
\|
\bm{C}
\|_{p,1}.
\end{split}
\end{equation}
Since the $\ell_{p,1}$ norm is separable with respect to each row, we can write the above problem as 
\begin{equation}
\begin{split}
\label{e6}
\min_{\bm{C}}
\sum_{i=1}^{M}
\frac{1}{2}
\|
(\bm{A}_{k+1})_{i*}+\rho^{-1}(\bm{\Delta}_{k})_{i*}
-
\bm{C}_{i*}
\|_{2}^{2}
+
\frac{\lambda}{\rho}
\|
\bm{C}_{i*}
\|_{p},
\end{split}
\end{equation}
where $\bm{F}_{i*}$ is the $i$th row of the matrix $\bm{F}$. Since this step is separable w.r.t. each row, we can employ parallel processing resources and reduce its computational time.


(3) Obtain the Lagrangian multipliers ($\bm{\Delta}_{k+1}$) while fixing ($\bm{C}_{k+1},\bm{A}_{k+1}$):
\begin{equation}
\begin{split}
\label{e10}
\bm{\Delta}_{k+1}
&
=
\bm{\Delta}_{k}
+
\rho
(\bm{C}_{k+1}-\bm{A}_{k+1})
.
\end{split}
\end{equation}

(5) Update the penalty parameter $\rho$ as $\rho=\mu\rho$, where $\mu>1$.


\section{Large Scale Subset Selection Via Row-Column Separation}



We can also restrict that $\text{diag}(\bm{A})=\bm{0}$ to avoid the samples to be self-represented. However, I want to mention that the proposed model solved by ADMM algorithm with three variables and does not have convergence results.

Then the model above can be 
\begin{equation}
\label{e2}
\min_{\bm{A}}
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{2}
+
\lambda
\|
\bm{A}
\|_{p,1}
\quad
\text{s.t.}
\quad
\text{diag}(\bm{A})=\bm{0}.
\end{equation}
By introducing an auxiliary variable $\bm{C}$ into the optimization program, we can get
\begin{equation}
\begin{split}
\label{e3}
&
\min_{\bm{A},\bm{C}}
\|
\bm{X}
-
\bm{X}\bm{C}
\|_{F}^{2}
+
\lambda
\|
\bm{A}
\|_{p,1}
\\
& 
\text{s.t.}
\ 
\bm{C}=\bm{A}-\text{diag}(\bm{A}),
\end{split}
\end{equation}
whose solution for $\bm{A}$ coincides with the solution of Eq. (\ref{e3}). By introducing two Lagrangian multipliers $\bm{\Delta}$, the Lagrangian function of the Eq. (\ref{e3}) can be written as
\begin{equation}
\begin{split}
\label{e5}
&
\mathcal{L}
(\bm{A},\bm{C},\bm{\Delta},\rho)
=
\|
\bm{X}
-
\bm{X}\bm{C}
\|_{F}^{2}
+
\lambda
\|
\bm{A}
\|_{p,1}
\\
&
+
\langle
\bm{\Delta},
\bm{C}-(\bm{A}-\text{diag}(\bm{A}))
\rangle
+
\frac{\rho}{2}
\|
\bm{C}-(\bm{A}-\text{diag}(\bm{A})
\|_{F}^{2}
\end{split}
\end{equation}
Denote by ($\bm{C}_{k},\bm{A}_{k}$) the optimization variables at iteration $k$, by $\bm{\Delta}_{k}$ the Lagrangian multipliers at iteration $k$, and by $\rho$ the penalty parameter at iteration $k$. Taking detivatives of $\mathcal{L}$ with respect to the variables and setting the derivatives to be zeros, we can alternatively update the variables as follows:

(1) Obtain $\bm{A}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{A}$, while fixing ($\bm{C}_{k},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e8}
&
\bm{A}_{k+1}
=
\bm{J}-\text{diag}(\bm{J}),
\\
&
\bm{J}
=
\arg\min_{\bm{J}}
\frac{1}{2}
\|
\bm{C}_{k}
+
\rho^{-1}\bm{\Delta}_{k}
-
\bm{J}
\|_{F}^{2}
+
\frac{\lambda}{\rho}
\|
\bm{J}
\|_{p,1}.
\end{split}
\end{equation}

(2) Obtain $\bm{C}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{C}$, while fixing ($\bm{A}_{k+1},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e6}
\min_{\bm{C}}
\|
\bm{X}-\bm{X}\bm{C}
\|_{F}^{2}
+
\frac{\rho}{2}
\|
\bm{C}-\bm{A}_{k+1}+\frac{1}{\rho}\bm{\Delta}_{k}
\|_{F}^{2}
\end{split}
\end{equation}
This is a least squares regression problem which has a closed-form solution as 
\begin{equation}
\begin{split}
\label{e7}
\bm{C}_{k+1} 
&
=
(\bm{X}^{\top}\bm{X}+\frac{\rho}{2}\bm{I})^{-1}
(\bm{X}^{\top}\bm{X}+\frac{\rho}{2}\bm{A}_{k+1}-\frac{1}{2}\bm{\Delta}_{k})
.
\end{split}
\end{equation}

(3) Obtain the Lagrangian multipliers ($\bm{\Delta}_{k+1}$) while fixing ($\bm{C}_{k+1},\bm{A}_{k+1}$):
\begin{equation}
\begin{split}
\label{e10}
\bm{\Delta}_{k+1}
&
=
\bm{\Delta}_{k}
+
\rho
(\bm{C}_{k+1}-\bm{A}_{k+1})
.
\end{split}
\end{equation}


\section{Experiments}

In this section, we apply the proposed Non-negative Least Squares Regression (NNLSR) model into subspace clustering, subset selection, and image classification problems.


\subsection{Subspae Clustering}

We evaluate the advantages of the proposed NNLSR method on subspace clustering problem. We compare with the state-of-the-art methods on four datasets including the Hopkins 155 dataset, the Extended Yale B dataset, the USPS and MNIST datasets. 

\begin{itemize}
\item The Hopkins 155 motion dataset contains 156 video sequences, 155 of which have two or three moving objects, and 1 video sequence has 5 moving onbjects. The motion trajectives of each object is viewed as a subspace. 

\item Extended Yale B dataset contains 38 human subjects, each subject has around 64 near frontal images taken under different illuminational conditions. The images are projected on a 60 dimensions space by PCA. Each class or subspace includes 64 imgaes which are resized into $32\times32$ pixels. 

\item The USPS contains 9298 images for digit numbers form 0 to 9. Each of the image in USPS is resized into $16\times16$ pixels. We use all the images in USPS for experiments.

\item The MNIST contains 60000 training images for digit numbers form 0 to 9.
\end{itemize}


\subsection{Subste Selection}





\subsection{Image Classification}
We compare the proposed method with ScSPM \cite{} and LLC \cite{} on UIUC psorts dataset \cite{} and Scene15 \cite{} dataset. 


\subsection{Complexity Analysis and Comparison}

{
\small
\bibliographystyle{unsrt}
\bibliography{egbib}
}

\end{document}
