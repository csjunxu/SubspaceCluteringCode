
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[numbers,sort]{natbib}
%\usepackage[UTF8]{ctex}


\usepackage{subfigure}
\usepackage{upgreek}
\usepackage{multirow}
\usepackage{color}
\usepackage{bm}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{arydshln}
\usepackage{latexsym}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conj}[theorem]{Conjecture}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}




\begin{document}

%%%%%%%%% TITLE
\title{Positive Collaborative Representation for Subspace Clustering}

\maketitle



%%%%%%%%% ABSTRACT
\begin{abstract}

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}



\section{Motivation}

\begin{itemize}
\item Positive collaborative representation could achieve sparse representation since similar points are sparse while dissimilar points are dense.

\item Positive supports are positive to self-reppresentation while negative supports are negative to self-representation.

\item Better performance. Faster? 
\end{itemize}

\section{LSR Model}
The least squares regression (LSR) model \cite{lsr} is proposed by Lu et al. can be formulated as follows:
\begin{equation}
\label{e1}
\min_{\bm{A}}
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{2}
+
\lambda
\|
\bm{A}
\|_{F}^{2}
\ 
\text{s.t.}
\ 
\text{diag}(\bm{A})=\bm{0}.
\end{equation}
Here we denote by $\text{diag}(\bm{A})$ both a diagonal matrix whose diagonal elements are the diagonal entries of $\bm{A}$ and the vector consisted of the diagonal elements. According to \cite{lsr}, the above problem has the optimal solution as 
\begin{equation}
\label{e2}
\hat{\bm{A}}
=
-\bm{\bm{Z}}(\text{diag}(\bm{Z}))
\
\text{s.t.} 
\
\text{diag}(\hat{\bm{A}})=\bm{0}
,
\end{equation}
where $\bm{Z}=(\bm{X}^{\top}\bm{X}+\lambda\bm{I})^{-1}$.

The constraint of  $\text{diag}(\bm{A})=\bm{0}$ in (\ref{e1}) could be removed and the LSR model achieves similar performance.


\section{Collaborative Representation based Clustering with Constraint $\text{diag}(\bm{A})=\bm{0}$}
The LSR model can be reformulated as a collaborative representation model \cite{crc} for subspace clustering with an additional constraint of $\text{diag}(\bm{A})=\bm{0}$. The constraint of $\text{diag}(\bm{A})=\bm{0}$ is used to avoid the samples to represent themselves.

By introducing auxiliary variables into the optimization program, we can set 
$
\bm{C}
=
\bm{A}
$.
The LSR model (\ref{e1}) can be transformed into
\begin{equation}
\begin{split}
\label{e3}
&
\min_{\bm{A},\bm{C}}
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{F}
+
\lambda
\|
\bm{C}
\|_{F}^{2}
\\
& 
\text{s.t.}
\ 
\bm{C}=\bm{A}-\text{diag}(\bm{A})
,
\end{split}
\end{equation}
whose solution for $\bm{A}$ coincides with the solution of Eq. (\ref{e1}). By introducing a Lagrangian multipliers $\bm{\Delta}$ and a penalty parameter $\rho$, the Lagrangian function of the Eq. (\ref{e3}) can be written as
\begin{equation}
\begin{split}
\label{e4}
&
\mathcal{L}
(\bm{A},\bm{C},\bm{\Delta},\rho)
=
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{F}
+
\lambda
\|
\bm{C}
\|_{F}^{2}
\\
&
+
\langle
\bm{\Delta},
\bm{C}-(\bm{A}-\text{diag}(\bm{A}))
\rangle
+
\frac{\rho}{2}
\|
\bm{C}-(\bm{A}-\text{diag}(\bm{A}))
\|_{F}^{2}
\end{split}
\end{equation}
Denote by ($\bm{C}_{k},\bm{A}_{k}$) the optimization variables at iteration $k$, by $\bm{\Delta}_{k}$ the Lagrangian multipliers at iteration $k$, and by $\rho$ the penalty parameter at iteration $k$. Taking detivatives of $\mathcal{L}$ with respect to the variables and setting the derivatives to be zeros, we can alternatively update the variables as follows:

(1) Obtain $\bm{A}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{A}$, while fixing ($\bm{C}_{k},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e8}
&
\bm{A}_{k+1}
=
\bm{J}-\text{diag}(\bm{J}),
\\
&
\bm{J}
=
(\bm{X}^{\top}\bm{X}+\frac{\rho}{2}\bm{I})^{-1}
(\bm{X}^{\top}\bm{X}+\frac{\rho}{2}\bm{C}_{k}+\frac{1}{2}\bm{\Delta}_{k})
\end{split}
\end{equation}

(2) Obtain $\bm{C}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{C}$, while fixing ($\bm{A}_{k+1},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e6}
&
\bm{C}_{k+1}
=
\arg\min_{\bm{C}}
\frac{\rho}{2}
\|
\bm{C}-(\bm{A}_{k+1}-\rho^{-1}\bm{\Delta}_{k})
\|_{F}^{2}
+
\lambda
\|
\bm{C}
\|_{F}^{2}
\end{split}
\end{equation}
This is a least squares regression problem which has a closed-form solution as 
\begin{equation}
\begin{split}
\label{e7}
\bm{C}_{k+1} 
=
(\rho+2\lambda)^{-1}(\rho\bm{A}_{k+1}-\bm{\Delta}_{k}).
\end{split}
\end{equation}

(3) Obtain the Lagrangian multipliers $\bm{\Delta}_{k+1}$ while fixing ($\bm{C}_{k+1},\bm{A}_{k+1}$):
\begin{equation}
\begin{split}
\label{e10}
\bm{\Delta}_{k+1}
&
=
\bm{\Delta}_{k}
+
\rho
(\bm{C}_{k+1}-\bm{A}_{k+1})
.
\end{split}
\end{equation}

\textbf{Convergency analysis?}

\section{Non-Negatie Collaborative Representation}

This model enforces non-negative representation and hence produce sparse solutions, in the sense that it results only a few non-negative coefficients.

The performance of this method is much better than the original least squares regression (LSR) based subspace clustering method proposed by Lu et al. \cite{lsr}.

The LSR model in \cite{lsr} can be reformulated as a collaborative representation model \cite{crc} for subspace clustering with an additional constraint of $\text{diag}(\bm{A})=\bm{0}$. In this section, we want to mention that the coefficient matrix $\bm{C}$ with additional constraint could benefit the performance of subspace clustering. Motivated by the non-negative coefficient should share positive relationship while negative coefficients share negative relationship, we argue that non-negative representational coefficients should better represent the data points from the same subspace, while negative coefficients correspond to points from different subspaces. By this way, the negative coefficients will negatively influence the relationship among the points in the same subsapce and hence degrade the performance of the model on subspace clustering. Based on these observations, in this section, we propose to add an constraint on the coefficient matrix $\bm{A}$ that the elements in $\bm{A}$ should be non-negative, i.e., $\bm{A}>0$. Hence, the proposed non-negative collaborative representation model can be formulated as follows: 
\begin{equation}
\min_{\bm{A}}
\end{equation}



By introducing auxiliary variables into the optimization program, we can set 
$
\bm{C}
=
\bm{A}
$.
The LSR model (\ref{e1}) can be transformed into
\begin{equation}
\begin{split}
\label{e3}
&
\min_{\bm{A},\bm{C}}
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{F}
+
\lambda
\|
\bm{C}
\|_{F}^{2}
\\
& 
\text{s.t.}
\ 
\bm{C}=\bm{A}-\text{diag}(\bm{A})
,
\end{split}
\end{equation}
whose solution for $\bm{A}$ coincides with the solution of Eq. (\ref{e1}). By introducing a Lagrangian multipliers $\bm{\Delta}$ and a penalty parameter $\rho$, the Lagrangian function of the Eq. (\ref{e3}) can be written as
\begin{equation}
\begin{split}
\label{e4}
&
\mathcal{L}
(\bm{A},\bm{C},\bm{\Delta},\rho)
=
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{F}
+
\lambda
\|
\bm{C}
\|_{F}^{2}
\\
&
+
\langle
\bm{\Delta},
\bm{C}-(\bm{A}-\text{diag}(\bm{A}))
\rangle
+
\frac{\rho}{2}
\|
\bm{C}-(\bm{A}-\text{diag}(\bm{A}))
\|_{F}^{2}
\end{split}
\end{equation}
Denote by ($\bm{C}_{k},\bm{A}_{k}$) the optimization variables at iteration $k$, by $\bm{\Delta}_{k}$ the Lagrangian multipliers at iteration $k$, and by $\rho$ the penalty parameter at iteration $k$. Taking detivatives of $\mathcal{L}$ with respect to the variables and setting the derivatives to be zeros, we can alternatively update the variables as follows:

(1) Obtain $\bm{A}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{A}$, while fixing ($\bm{C}_{k},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e8}
&
\bm{A}_{k+1}
=
\bm{J}-\text{diag}(\bm{J}),
\\
&
\bm{J}
=
(\bm{X}^{\top}\bm{X}+\frac{\rho}{2}\bm{I})^{-1}
(\bm{X}^{\top}\bm{X}+\frac{\rho}{2}\bm{C}_{k}+\frac{1}{2}\bm{\Delta}_{k})
\end{split}
\end{equation}

(2) Obtain $\bm{C}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{C}$, while fixing ($\bm{A}_{k+1},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e6}
&
\bm{C}_{k+1}
=
\arg\min_{\bm{C}}
\frac{\rho}{2}
\|
\bm{C}-(\bm{A}_{k+1}-\rho^{-1}\bm{\Delta}_{k})
\|_{F}^{2}
+
\lambda
\|
\bm{C}
\|_{F}^{2}
\end{split}
\end{equation}
This is a least squares regression problem which has a closed-form solution as 
\begin{equation}
\begin{split}
\label{e7}
\bm{C}_{k+1} 
=
(\rho+2\lambda)^{-1}(\rho\bm{A}_{k+1}-\bm{\Delta}_{k}).
\end{split}
\end{equation}

(3) Obtain the Lagrangian multipliers $\bm{\Delta}_{k+1}$ while fixing ($\bm{C}_{k+1},\bm{A}_{k+1}$):
\begin{equation}
\begin{split}
\label{e10}
\bm{\Delta}_{k+1}
&
=
\bm{\Delta}_{k}
+
\rho
(\bm{C}_{k+1}-\bm{A}_{k+1})
.
\end{split}
\end{equation}

\textbf{Convergency analysis?}






\section{Large Scale Subset Selection Via Woodbury Identity}

The Woodbury Identity is 
\begin{equation}
(
\bm{A}
+
\bm{U}
\bm{C}
\bm{V}
)^{-1}
=
\bm{A}^{-1}
-
\bm{A}^{-1}
\bm{U}
(
\bm{C}^{-1}
+
\bm{V}\bm{A}^{-1}\bm{U}
)^{-1}
\bm{V}\bm{A}^{-1}
.
\end{equation}



We can also restrict that $\text{diag}(\bm{A})=\bm{0}$ to avoid the samples to be self-represented. However, I want to mention that the proposed model solved by ADMM algorithm with three variables and does not have convergence results.

Then the model above can be 
\begin{equation}
\label{e2}
\min_{\bm{A}}
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{2}
+
\lambda
\|
\bm{A}
\|_{p,1}
.
\end{equation}
By introducing an auxiliary variable $\bm{C}$ into the optimization program, we can get
\begin{equation}
\begin{split}
\label{e3}
&
\min_{\bm{A},\bm{C}}
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{2}
+
\lambda
\|
\bm{C}
\|_{p,1}
\ 
\text{s.t.}
\ 
\bm{C}=\bm{A}
,
\end{split}
\end{equation}
whose solution for $\bm{A}$ coincides with the solution of Eq. (\ref{e3}). By introducing two Lagrangian multipliers $\bm{\Delta}$, the Lagrangian function of the Eq. (\ref{e3}) can be written as
\begin{equation}
\begin{split}
\label{e5}
&
\mathcal{L}
(\bm{A},\bm{C},\bm{\Delta},\rho)
=
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{2}
+
\lambda
\|
\bm{C}
\|_{p,1}
\\
&
+
\langle
\bm{\Delta},
\bm{C}-\bm{A}
\rangle
+
\frac{\rho}{2}
\|
\bm{C}-\bm{A}
\|_{F}^{2}
\end{split}
\end{equation}
Denote by ($\bm{C}_{k},\bm{A}_{k}$) the optimization variables at iteration $k$, by $\bm{\Delta}_{k}$ the Lagrangian multipliers at iteration $k$, and by $\rho$ the penalty parameter at iteration $k$. Taking detivatives of $\mathcal{L}$ with respect to the variables and setting the derivatives to be zeros, we can alternatively update the variables as follows:

(1) Obtain $\bm{A}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{A}$, while fixing ($\bm{C}_{k},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e8}
&
\min_{\bm{A}}
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{2}
+
\frac{\rho}{2}
\|
\bm{A}
-(
\bm{C}_{k}
+
\rho^{-1}
\bm{\Delta}_{k}
)
\|_{F}^{2},
\end{split}
\end{equation}
which is equalivalently to solve the following problem
\begin{equation}
\begin{split}
\label{e8}
\bm{A}
=
(\bm{X}^{\top}\bm{X}+\frac{\rho}{2}\bm{I})^{-1}
(\bm{X}^{\top}\bm{X}
+
\frac{\rho}{2}
\bm{C}_{k}
+
\frac{1}{2}
\bm{\Delta}_{k}
)
\end{split}
\end{equation}
Since the matrices $\bm{X}^{\top}\bm{X}$ is of $N\times N$ dimension. It is computational expensive when $N$ is very large. By employing the Woodburry Identity mentioned above, we can have 
\begin{equation}
(
\frac{\rho}{2}\bm{I}
+
\bm{X}^{\top}\bm{X}
)^{-1}
=
\frac{2}{\rho}\bm{I}
-
(\frac{2}{\rho})^{2}
\bm{X}^{\top}
(
\bm{I}
+
\frac{2}{\rho}\bm{X}\bm{X}^{\top}
)^{-1}
\bm{X}
.
\end{equation}
and transform this problem as
\begin{equation}
\begin{split}
\label{e8}
\bm{A}
=
&
(
\frac{2}{\rho}\bm{I}
-
(\frac{2}{\rho})^{2}
\bm{X}^{\top}
(
\bm{I}
+
\frac{2}{\rho}\bm{X}\bm{X}^{\top}
)^{-1}
\bm{X}
)
\\
&
*
(\bm{X}^{\top}\bm{X}
+
\frac{\rho}{2}
\bm{C}_{k}
+
\frac{1}{2}
\bm{\Delta}_{k}
)
\end{split}
\end{equation}
which will save a lot of computational costs.


(2) Obtain $\bm{C}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{C}$, while fixing ($\bm{A}_{k+1},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e6}
\min_{\bm{C}}
\frac{1}{2}
\|
(\bm{A}_{k+1}-\rho^{-1}\bm{\Delta}{k})
-
\bm{C}
\|_{F}^{2}
+
\frac{\lambda}{\rho}
\|
\bm{C}
\|_{p,1}.
\end{split}
\end{equation}
Since the $\ell_{p,1}$ norm is separable with respect to each row, we can write the above problem as 
\begin{equation}
\begin{split}
\label{e6}
\min_{\bm{C}}
\sum_{i=1}^{M}
\frac{1}{2}
\|
(\bm{A}_{k+1})_{i*}-\rho^{-1}(\bm{\Delta}_{k})_{i*}
-
\bm{C}_{i*}
\|_{2}^{2}
+
\frac{\lambda}{\rho}
\|
\bm{C}_{i*}
\|_{p},
\end{split}
\end{equation}
where $\bm{F}_{i*}$ is the $i$th row of the matrix $\bm{F}$. Since this step is separable w.r.t. each row, we can employ parallel processing resources and reduce its computational time.


(3) Obtain the Lagrangian multipliers ($\bm{\Delta}_{k+1}$) while fixing ($\bm{C}_{k+1},\bm{A}_{k+1}$):
\begin{equation}
\begin{split}
\label{e10}
\bm{\Delta}_{k+1}
&
=
\bm{\Delta}_{k}
+
\rho
(\bm{C}_{k+1}-\bm{A}_{k+1})
.
\end{split}
\end{equation}

(5) Update the penalty parameter $\rho$ as $\rho=\mu\rho$, where $\mu>1$.


\section{Robust Large Scale Subset Selection via Dissimilarity based Outlier Detection}


We can also introduce a dissimiarlity based matrix $\bm{D}$ to replace the $\ell_{p}$ or $\ell_{2,1}$ norms to ensure robustness. This can also remore the additional term $\bm{Z}$ on modeling the outliers with the restriction of $\ell_{1}$ norm. The matrix $\bm{D}$ should better be diagonal matrix. How to design the matrix $\bm{D}$ is another problem need to be solved.

Then the proposed model can be formulated as  
\begin{equation}
\label{e2}
\min_{\bm{A}}
\|
(\bm{X}
-
\bm{X}\bm{A})
\bm{D}
\|_{F}^{2}
+
\lambda
\|
\bm{A}
\|_{p,1}
.
\end{equation}
By introducing an auxiliary variable $\bm{C}$ into the optimization program, we can get
\begin{equation}
\begin{split}
\label{e3}
&
\min_{\bm{A},\bm{C}}
\|
(\bm{X}
-
\bm{X}\bm{A})
\bm{D}
\|_{F}^{2}
+
\lambda
\|
\bm{C}
\|_{p,1}
\ 
\text{s.t.}
\ 
\bm{C}=\bm{A}
.
\end{split}
\end{equation}
By introducing a Lagrangian multiplier $\bm{\Delta}$, the Lagrangian function of the Eq. (\ref{e3}) can be written as
\begin{equation}
\begin{split}
\label{e5}
&
\mathcal{L}
(\bm{A},\bm{C},\bm{\Delta},\rho)
=
\|
(\bm{X}
-
\bm{X}\bm{A})
\bm{D}
\|_{F}^{2}
+
\lambda
\|
\bm{C}
\|_{p,1}
\\
&
+
\langle
\bm{\Delta},
\bm{C}-\bm{A}
\rangle
+
\frac{\rho}{2}
\|
\bm{C}
-
\bm{A}
\|_{F}^{2}
\end{split}
\end{equation}
Denote by ($\bm{A}_{k},\bm{C}_{k}$) the optimization variables at iteration $k$, by $\bm{\Delta}_{k}$ the Lagrangian multiplier at iteration $k$, and by $\rho$ the penalty parameter at iteration $k$. Taking detivatives of $\mathcal{L}$ with respect to the variables and setting the derivatives to be zeros, we can alternatively update the variables as follows:

(1) Obtain $\bm{A}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{A}$, while fixing ($\bm{C}_{k},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e8}
&
\min_{\bm{A}}
\|
(\bm{X}
-
\bm{X}\bm{A})
\bm{D}
\|_{F}^{2}
+
\frac{\rho}{2}
\|
\bm{A}
-(
\bm{C}_{k}
-
\rho^{-1}
\bm{\Delta}_{k}
)
\|_{F}^{2},
\end{split}
\end{equation}
which is equalivalently to solve the following problem
\begin{equation}
\begin{split}
\label{e8}
\bm{X}^{\top}\bm{X}\bm{A}\bm{D}\bm{D}^{\top}
+
\frac{\rho}{2}
\bm{A}
=
\bm{X}^{\top}\bm{X}\bm{D}\bm{D}^{\top}
+
\frac{\rho}{2}
(
\bm{C}_{k}
-
\rho^{-1}
\bm{\Delta}_{k}
)
\end{split}
\end{equation}
Since the matrices $\bm{X}^{\top}\bm{X}$ and $\bm{D}^{\top}\bm{D}$ are positive semi-definite and positive definite, respectively. The above equation is a standard Sylvester equation which has a unique solution.


(2) Obtain $\bm{C}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{C}$, while fixing ($\bm{A}_{k+1},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e6}
\min_{\bm{C}}
\frac{1}{2}
\|
(\bm{A}_{k+1}+\rho^{-1}\bm{\Delta}{k})
-
\bm{C}
\|_{F}^{2}
+
\frac{\lambda}{\rho}
\|
\bm{C}
\|_{p,1}.
\end{split}
\end{equation}
Since the $\ell_{p,1}$ norm is separable with respect to each row, we can write the above problem as 
\begin{equation}
\begin{split}
\label{e6}
\min_{\bm{C}}
\sum_{i=1}^{M}
\frac{1}{2}
\|
(\bm{A}_{k+1})_{i*}+\rho^{-1}(\bm{\Delta}_{k})_{i*}
-
\bm{C}_{i*}
\|_{2}^{2}
+
\frac{\lambda}{\rho}
\|
\bm{C}_{i*}
\|_{p},
\end{split}
\end{equation}
where $\bm{F}_{i*}$ is the $i$th row of the matrix $\bm{F}$. Since this step is separable w.r.t. each row, we can employ parallel processing resources and reduce its computational time.


(3) Obtain the Lagrangian multipliers ($\bm{\Delta}_{k+1}$) while fixing ($\bm{C}_{k+1},\bm{A}_{k+1}$):
\begin{equation}
\begin{split}
\label{e10}
\bm{\Delta}_{k+1}
&
=
\bm{\Delta}_{k}
+
\rho
(\bm{C}_{k+1}-\bm{A}_{k+1})
.
\end{split}
\end{equation}

(5) Update the penalty parameter $\rho$ as $\rho=\mu\rho$, where $\mu>1$.


\section{Large Scale Subset Selection Via Row-Column Separation}



We can also restrict that $\text{diag}(\bm{A})=\bm{0}$ to avoid the samples to be self-represented. However, I want to mention that the proposed model solved by ADMM algorithm with three variables and does not have convergence results.

Then the model above can be 
\begin{equation}
\label{e2}
\min_{\bm{A}}
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{2}
+
\lambda
\|
\bm{A}
\|_{p,1}
\quad
\text{s.t.}
\quad
\text{diag}(\bm{A})=\bm{0}.
\end{equation}
By introducing an auxiliary variable $\bm{C}$ into the optimization program, we can get
\begin{equation}
\begin{split}
\label{e3}
&
\min_{\bm{A},\bm{C}}
\|
\bm{X}
-
\bm{X}\bm{C}
\|_{F}^{2}
+
\lambda
\|
\bm{A}
\|_{p,1}
\\
& 
\text{s.t.}
\ 
\bm{C}=\bm{A}-\text{diag}(\bm{A}),
\end{split}
\end{equation}
whose solution for $\bm{A}$ coincides with the solution of Eq. (\ref{e3}). By introducing two Lagrangian multipliers $\bm{\Delta}$, the Lagrangian function of the Eq. (\ref{e3}) can be written as
\begin{equation}
\begin{split}
\label{e5}
&
\mathcal{L}
(\bm{A},\bm{C},\bm{\Delta},\rho)
=
\|
\bm{X}
-
\bm{X}\bm{C}
\|_{F}^{2}
+
\lambda
\|
\bm{A}
\|_{p,1}
\\
&
+
\langle
\bm{\Delta},
\bm{C}-(\bm{A}-\text{diag}(\bm{A}))
\rangle
+
\frac{\rho}{2}
\|
\bm{C}-(\bm{A}-\text{diag}(\bm{A})
\|_{F}^{2}
\end{split}
\end{equation}
Denote by ($\bm{C}_{k},\bm{A}_{k}$) the optimization variables at iteration $k$, by $\bm{\Delta}_{k}$ the Lagrangian multipliers at iteration $k$, and by $\rho$ the penalty parameter at iteration $k$. Taking detivatives of $\mathcal{L}$ with respect to the variables and setting the derivatives to be zeros, we can alternatively update the variables as follows:

(1) Obtain $\bm{A}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{A}$, while fixing ($\bm{C}_{k},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e8}
&
\bm{A}_{k+1}
=
\bm{J}-\text{diag}(\bm{J}),
\\
&
\bm{J}
=
\arg\min_{\bm{J}}
\frac{1}{2}
\|
\bm{C}_{k}
+
\rho^{-1}\bm{\Delta}_{k}
-
\bm{J}
\|_{F}^{2}
+
\frac{\lambda}{\rho}
\|
\bm{J}
\|_{p,1}.
\end{split}
\end{equation}

(2) Obtain $\bm{C}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{C}$, while fixing ($\bm{A}_{k+1},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e6}
\min_{\bm{C}}
\|
\bm{X}-\bm{X}\bm{C}
\|_{F}^{2}
+
\frac{\rho}{2}
\|
\bm{C}-\bm{A}_{k+1}+\frac{1}{\rho}\bm{\Delta}_{k}
\|_{F}^{2}
\end{split}
\end{equation}
This is a least squares regression problem which has a closed-form solution as 
\begin{equation}
\begin{split}
\label{e7}
\bm{C}_{k+1} 
&
=
(\bm{X}^{\top}\bm{X}+\frac{\rho}{2}\bm{I})^{-1}
(\bm{X}^{\top}\bm{X}+\frac{\rho}{2}\bm{A}_{k+1}-\frac{1}{2}\bm{\Delta}_{k})
.
\end{split}
\end{equation}

(3) Obtain the Lagrangian multipliers ($\bm{\Delta}_{k+1}$) while fixing ($\bm{C}_{k+1},\bm{A}_{k+1}$):
\begin{equation}
\begin{split}
\label{e10}
\bm{\Delta}_{k+1}
&
=
\bm{\Delta}_{k}
+
\rho
(\bm{C}_{k+1}-\bm{A}_{k+1})
.
\end{split}
\end{equation}

(5) Update the penalty parameter $\rho$ as $\rho=\mu\rho$, where $\mu>1$.


{
\small
\bibliographystyle{unsrt}
\bibliography{egbib}
}

\end{document}
