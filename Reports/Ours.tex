
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[numbers,sort]{natbib}
%\usepackage[UTF8]{ctex}


\usepackage{subfigure}
\usepackage{upgreek}
\usepackage{multirow}
\usepackage{color}
\usepackage{bm}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{arydshln}
\usepackage{latexsym}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conj}[theorem]{Conjecture}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}




\begin{document}

%%%%%%%%% TITLE
\title{Positive Collaborative Representation for Subspace Clustering}

\maketitle



%%%%%%%%% ABSTRACT
\begin{abstract}

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}



\section{Motivation}

\begin{itemize}
\item Positive collaborative representation could achieve sparse representation since similar points are sparse while dissimilar points are dense.

\item Positive supports are positive to self-reppresentation while negative supports are negative to self-representation.

\item Better performance. Faster? 
\end{itemize}

\section{LSR Model}
The least squares regression (LSR) model \cite{lu2012robust} is proposed by Lu et al. can be formulated as follows:
\begin{equation}
\label{e1}
\min_{\bm{A}}
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{2}
+
\lambda
\|
\bm{A}
\|_{F}^{2}
\ 
\text{s.t.}
\ 
\text{diag}(\bm{A})=\bm{0}.
\end{equation}
According to \cite{lu2012robust}, the above problem has the optimal solution as 
\begin{equation}
\label{e2}
\hat{\bm{A}}
=
-\bm{\bm{Z}}(\text{diag}(\bm{Z}))
\
\text{s.t.} 
\
\text{diag}(\hat{\bm{A}})=\bm{0}
,
\end{equation}
where $\bm{Z}=(\bm{X}^{\top}\bm{X}+\lambda\bm{I})^{-1}$.

The constraint of  $\text{diag}(\bm{A})=\bm{0}$ in (\ref{e1}) could be removed and the LSR model achieves similar performance.


\section{Collaborative Representation based Clustering with Constraint $\text{diag}(\bm{A})=\bm{0}$}
The LSR model can be reformulated as a collaborative representation model \cite{crc} for subspace clustering with an additional constraint of $\text{diag}(\bm{A})=\bm{0}$. The constraint of $\text{diag}(\bm{A})=\bm{0}$ is used to avoid the samples to be self-represented.

Then the model above can be 
\begin{equation}
\label{e2}
\min_{\bm{A}}
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{p}^{p}
+
\lambda
\|
\bm{A}
\|_{F}^{2}
\quad
\text{s.t.}
\quad
\text{diag}(\bm{A})=\bm{0}.
\end{equation}

By introducing auxiliary variables into the optimization program, we can set 
$
\bm{E}
=
\bm{X}
-
\bm{X}\bm{A}
$ 
and 
$
\bm{A}
=
\bm{0}
$.
The program (\ref{e2}) can be transformed into
\begin{equation}
\begin{split}
\label{e3}
&
\min_{\bm{A},\bm{E}}
\|
\bm{E}
\|_{p}^{p}
+
\lambda
\|
\bm{A}
\|_{F}^{2}
\\
& 
\text{s.t.}
\ 
\bm{E}=\bm{X}-\bm{X}\bm{A}
,
\text{diag}(\bm{A})=\bm{0}.
\end{split}
\end{equation}
Besides, we can further introduce an auxiliary matrix variable $\bm{C}$ and consider the following program
\begin{equation}
\begin{split}
\label{e4}
&
\min_{\bm{A},\bm{C},\bm{E}}
\|
\bm{E}
\|_{p}^{p}
+
\lambda
\|
\bm{A}
\|_{F}^{2}
\\
& 
\text{s.t.}
\ 
\bm{E}=\bm{X}-\bm{X}\bm{C}
,
\bm{C}
=
\bm{A}
-
\text{diag}(\bm{A})
\end{split}
\end{equation}
whose solution for ($\bm{A},\bm{E}$) coincides with the solution of Eq. (\ref{e3}). By introducing two Lagrangian multipliers $\bm{\Delta},\bm{\delta}$, the Lagrangian function of the Eq. (\ref{e4}) can be written as
\begin{equation}
\begin{split}
\label{e5}
&
\mathcal{L}
(\bm{A},\bm{C},\bm{E},\bm{\Delta},\bm{\delta},\rho)
=
\|
\bm{E}
\|_{p}^{p}
+
\lambda
\|
\bm{A}
\|_{F}^{2}
\\
&
+
\frac{\rho}{2}
\|
\bm{E}-\bm{X}+\bm{X}\bm{C}+\rho^{-1}\bm{\Delta}
\|_{F}^{2}
\\
&
+
\frac{\rho}{2}
\|
\bm{C}-\bm{A}+\text{diag}(\bm{A})+\rho^{-1}\bm{\delta}
\|_{F}^{2}
\end{split}
\end{equation}
Denote by ($\bm{C}_{k},\bm{A}_{k},\bm{E}_{k}$) the optimization variables at iteration $k$, by ($\bm{\Delta}_{k},\bm{\delta}_{k}$) the Lagrangian multipliers at iteration $k$, and by $\rho_{k}$ the penalty parameter at iteration $k$. Taking detivatives of $\mathcal{L}$ with respect to the variables and setting the derivatives to be zeros, we can alternatively update the variables as follows:

(1) Obtain $\bm{A}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{A}$, while fixing ($\bm{C}_{k},\bm{E}_{k},\bm{\Delta}_{k},\bm{\delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e8}
&
\bm{A}_{k+1}
=
\bm{J}-\text{diag}(\bm{J}),
\\
&
\bm{J}
=
(\rho_{k}+2\lambda)^{-1}
(\rho_{k}\bm{C}_{k}+\bm{\delta}_{k})
\end{split}
\end{equation}

(2) Obtain $\bm{C}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{C}$, while fixing ($\bm{A}_{k+1},\bm{E}_{k},\bm{\Delta}_{k},\bm{\delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e6}
&
\bm{C}_{k+1}
=
\arg\min_{\bm{C}}
\frac{\rho_{k}}{2}
\|
\bm{E}_{k}-\bm{X}+\bm{X}\bm{C}+\rho_{k}^{-1}\bm{\Delta}_{k}
\|_{F}^{2}
\\
&
+
\frac{\rho_{k}}{2}
\|
\bm{C}-\bm{A}_{k+1}+\rho_{k}^{-1}\bm{\delta}_{k}
\|_{F}^{2}
\end{split}
\end{equation}
This is a least squares regression problem which has a closed-form solution as 
\begin{equation}
\begin{split}
\label{e7}
\bm{C}_{k+1} 
&
=
\bm{H}-\text{diag}(\bm{H}),
\\
(\bm{X}^{\top}\bm{X}+\bm{I})\bm{H}
&
=
\bm{X}^{\top}\bm{P}_{k}+\bm{Q}_{k},
\end{split}
\end{equation}
where $\bm{P}_{k}=\bm{X}-\bm{E}_{k}-\rho_{k}^{-1}\bm{\Delta}_{k}$
and
$\bm{Q}_{k}=\bm{A}_{k+1}-\rho_{k}^{-1}\bm{\delta}_{k}$.

(3) Obtain $\bm{E}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{E}$, while fixing ($\bm{C}_{k+1},\bm{A}_{k+1},\bm{\Delta}_{k},\bm{\delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e9}
&
\min_{\bm{E}}
\frac{1}{2}
\|
(\bm{X}-\bm{X}\bm{C}_{k+1}-\rho_{k}^{-1}\bm{\Delta}_{k})
-
\bm{E}
\|_{F}^{2}
+
\rho_{k}^{-1}
\|
\bm{E}
\|_{1}.
\end{split}
\end{equation}
The solution of $\bm{E}$ can be computed in closed-form as 
\begin{equation}
\begin{split}
\label{e10}
&
\bm{E}_{k+1}
=
\mathcal{S}_{\rho_{k}^{-1}}
(\bm{X}-\bm{X}\bm{C}_{k+1}-\rho_{k}^{-1}\bm{\Delta}_{k}),
\end{split}
\end{equation}
where $\mathcal{S}_{\tau}(x)=\text{sign}(x)*\text{max}(|x|-\tau,0)$ is the soft-thresholding operator.

(4) Obtain the Lagrangian multipliers ($\bm{\Delta}_{k+1},\bm{\delta}_{k+1}$) while fixing ($\bm{C}_{k+1},\bm{A}_{k+1},\bm{E}_{k+1}$):
\begin{equation}
\begin{split}
\label{e10}
\bm{\Delta}_{k+1}
&
=
\bm{\Delta}_{k}
+
\tau\rho_{k}
(\bm{E}_{k+1}-\bm{X}+\bm{X}\bm{C}_{k+1})
,
\\
\bm{\delta}_{k+1}
&
=
\bm{\delta}_{k}
+
\tau\rho_{k}
(\bm{C}_{k+1}-\bm{A}_{k+1})
,
\end{split}
\end{equation}
where $\tau\in(0,\frac{\sqrt{5}+1}{2})$ is the dual step size and is usually set as $\tau=1$.

(5) Update the penalty parameter $\rho$ as $\rho_{k+1}=\mu\rho_{k}$, where $\mu>1$.


\section{Large Scale Subset Selection Via Woodbury Identity}

The Woodbury Identity is 
\begin{equation}
(
\bm{A}
+
\bm{U}
\bm{C}
\bm{V}
)^{-1}
=
\bm{A}^{-1}
-
\bm{A}^{-1}
\bm{U}
(
\bm{C}^{-1}
+
\bm{V}\bm{A}^{-1}\bm{U}
)^{-1}
\bm{V}\bm{A}^{-1}
.
\end{equation}



We can also restrict that $\text{diag}(\bm{A})=\bm{0}$ to avoid the samples to be self-represented. However, I want to mention that the proposed model solved by ADMM algorithm with three variables and does not have convergence results.

Then the model above can be 
\begin{equation}
\label{e2}
\min_{\bm{A}}
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{2}
+
\lambda
\|
\bm{A}
\|_{p,1}
.
\end{equation}
By introducing an auxiliary variable $\bm{C}$ into the optimization program, we can get
\begin{equation}
\begin{split}
\label{e3}
&
\min_{\bm{A},\bm{C}}
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{2}
+
\lambda
\|
\bm{C}
\|_{p,1}
\ 
\text{s.t.}
\ 
\bm{C}=\bm{A}
,
\end{split}
\end{equation}
whose solution for $\bm{A}$ coincides with the solution of Eq. (\ref{e3}). By introducing two Lagrangian multipliers $\bm{\Delta}$, the Lagrangian function of the Eq. (\ref{e3}) can be written as
\begin{equation}
\begin{split}
\label{e5}
&
\mathcal{L}
(\bm{A},\bm{C},\bm{\Delta},\rho)
=
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{2}
+
\lambda
\|
\bm{C}
\|_{p,1}
\\
&
+
\langle
\bm{\Delta},
\bm{C}-\bm{A}
\rangle
+
\frac{\rho}{2}
\|
\bm{C}-\bm{A}
\|_{F}^{2}
\end{split}
\end{equation}
Denote by ($\bm{C}_{k},\bm{A}_{k}$) the optimization variables at iteration $k$, by $\bm{\Delta}_{k}$ the Lagrangian multipliers at iteration $k$, and by $\rho_{k}$ the penalty parameter at iteration $k$. Taking detivatives of $\mathcal{L}$ with respect to the variables and setting the derivatives to be zeros, we can alternatively update the variables as follows:

(1) Obtain $\bm{A}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{A}$, while fixing ($\bm{C}_{k},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e8}
&
\min_{\bm{A}}
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{2}
+
\frac{\rho}{2}
\|
\bm{A}
-(
\bm{C}_{k}
+
\rho_{k}^{-1}
\bm{\Delta}_{k}
)
\|_{F}^{2},
\end{split}
\end{equation}
which is equalivalently to solve the following problem
\begin{equation}
\begin{split}
\label{e8}
\bm{A}
=
(\bm{X}^{\top}\bm{X}+\frac{\rho_{k}}{2}\bm{I})^{-1}
(\bm{X}^{\top}\bm{X}
+
\frac{\rho_{k}}{2}
\bm{C}_{k}
+
\frac{1}{2}
\bm{\Delta}_{k}
)
\end{split}
\end{equation}
Since the matrices $\bm{X}^{\top}\bm{X}$ is of $N\times N$ dimension. It is computational expensive when $N$ is very large. By employing the Woodburry Identity mentioned above, we can have 
\begin{equation}
(
\frac{\rho_{k}}{2}\bm{I}
+
\bm{X}^{\top}\bm{X}
)^{-1}
=
\frac{2}{\rho_{k}}\bm{I}
-
(\frac{2}{\rho_{k}})^{2}
\bm{X}^{\top}
(
\bm{I}
+
\frac{2}{\rho_{k}}\bm{X}\bm{X}^{\top}
)^{-1}
\bm{X}
.
\end{equation}
and transform this problem as
\begin{equation}
\begin{split}
\label{e8}
\bm{A}
=
&
(
\frac{2}{\rho_{k}}\bm{I}
-
(\frac{2}{\rho_{k}})^{2}
\bm{X}^{\top}
(
\bm{I}
+
\frac{2}{\rho_{k}}\bm{X}\bm{X}^{\top}
)^{-1}
\bm{X}
)
\\
&
*
(\bm{X}^{\top}\bm{X}
+
\frac{\rho_{k}}{2}
\bm{C}_{k}
+
\frac{1}{2}
\bm{\Delta}_{k}
)
\end{split}
\end{equation}
which will save a lot of computational costs.


(2) Obtain $\bm{C}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{C}$, while fixing ($\bm{A}_{k+1},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e6}
\min_{\bm{C}}
\frac{1}{2}
\|
(\bm{A}_{k+1}-\rho_{k}^{-1}\bm{\Delta}{k})
-
\bm{C}
\|_{F}^{2}
+
\frac{\lambda}{\rho_{k}}
\|
\bm{C}
\|_{p,1}.
\end{split}
\end{equation}
Since the $\ell_{p,1}$ norm is separable with respect to each row, we can write the above problem as 
\begin{equation}
\begin{split}
\label{e6}
\min_{\bm{C}}
\sum_{i=1}^{M}
\frac{1}{2}
\|
(\bm{A}_{k+1})_{i*}-\rho_{k}^{-1}(\bm{\Delta}_{k})_{i*}
-
\bm{C}_{i*}
\|_{2}^{2}
+
\frac{\lambda}{\rho_{k}}
\|
\bm{C}_{i*}
\|_{p},
\end{split}
\end{equation}
where $\bm{F}_{i*}$ is the $i$th row of the matrix $\bm{F}$. Since this step is separable w.r.t. each row, we can employ parallel processing resources and reduce its computational time.


(3) Obtain the Lagrangian multipliers ($\bm{\Delta}_{k+1}$) while fixing ($\bm{C}_{k+1},\bm{A}_{k+1}$):
\begin{equation}
\begin{split}
\label{e10}
\bm{\Delta}_{k+1}
&
=
\bm{\Delta}_{k}
+
\rho_{k}
(\bm{C}_{k+1}-\bm{A}_{k+1})
.
\end{split}
\end{equation}

(5) Update the penalty parameter $\rho$ as $\rho_{k+1}=\mu\rho_{k}$, where $\mu>1$.


\section{Robust Large Scale Subset Selection via Dissimilarity based Outlier Detection}


We can also introduce a dissimiarlity based matrix $\bm{D}$ to replace the $\ell_{p}$ or $\ell_{2,1}$ norms to ensure robustness. This can also remore the additional term $\bm{Z}$ on modeling the outliers with the restriction of $\ell_{1}$ norm. The matrix $\bm{D}$ should better be diagonal matrix. How to design the matrix $\bm{D}$ is another problem need to be solved.

Then the proposed model can be formulated as  
\begin{equation}
\label{e2}
\min_{\bm{A}}
\|
(\bm{X}
-
\bm{X}\bm{A})
\bm{D}
\|_{F}^{2}
+
\lambda
\|
\bm{A}
\|_{p,1}
.
\end{equation}
By introducing an auxiliary variable $\bm{C}$ into the optimization program, we can get
\begin{equation}
\begin{split}
\label{e3}
&
\min_{\bm{A},\bm{C}}
\|
(\bm{X}
-
\bm{X}\bm{A})
\bm{D}
\|_{F}^{2}
+
\lambda
\|
\bm{C}
\|_{p,1}
\ 
\text{s.t.}
\ 
\bm{C}=\bm{A}
.
\end{split}
\end{equation}
By introducing a Lagrangian multiplier $\bm{\Delta}$, the Lagrangian function of the Eq. (\ref{e3}) can be written as
\begin{equation}
\begin{split}
\label{e5}
&
\mathcal{L}
(\bm{A},\bm{C},\bm{\Delta},\rho)
=
\|
(\bm{X}
-
\bm{X}\bm{A})
\bm{D}
\|_{F}^{2}
+
\lambda
\|
\bm{C}
\|_{p,1}
\\
&
+
\langle
\bm{\Delta},
\bm{C}-\bm{A}
\rangle
+
\frac{\rho}{2}
\|
\bm{C}
-
\bm{A}
\|_{F}^{2}
\end{split}
\end{equation}
Denote by ($\bm{A}_{k},\bm{C}_{k}$) the optimization variables at iteration $k$, by $\bm{\Delta}_{k}$ the Lagrangian multiplier at iteration $k$, and by $\rho_{k}$ the penalty parameter at iteration $k$. Taking detivatives of $\mathcal{L}$ with respect to the variables and setting the derivatives to be zeros, we can alternatively update the variables as follows:

(1) Obtain $\bm{A}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{A}$, while fixing ($\bm{C}_{k},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e8}
&
\min_{\bm{A}}
\|
(\bm{X}
-
\bm{X}\bm{A})
\bm{D}
\|_{F}^{2}
+
\frac{\rho}{2}
\|
\bm{A}
-(
\bm{C}_{k}
-
\rho_{k}^{-1}
\bm{\Delta}_{k}
)
\|_{F}^{2},
\end{split}
\end{equation}
which is equalivalently to solve the following problem
\begin{equation}
\begin{split}
\label{e8}
\bm{X}^{\top}\bm{X}\bm{A}\bm{D}\bm{D}^{\top}
+
\frac{\rho_{k}}{2}
\bm{A}
=
\bm{X}^{\top}\bm{X}\bm{D}\bm{D}^{\top}
+
\frac{\rho_{k}}{2}
(
\bm{C}_{k}
-
\rho_{k}^{-1}
\bm{\Delta}_{k}
)
\end{split}
\end{equation}
Since the matrices $\bm{X}^{\top}\bm{X}$ and $\bm{D}^{\top}\bm{D}$ are positive semi-definite and positive definite, respectively. The above equation is a standard Sylvester equation which has a unique solution.


(2) Obtain $\bm{C}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{C}$, while fixing ($\bm{A}_{k+1},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e6}
\min_{\bm{C}}
\frac{1}{2}
\|
(\bm{A}_{k+1}+\rho_{k}^{-1}\bm{\Delta}{k})
-
\bm{C}
\|_{F}^{2}
+
\frac{\lambda}{\rho_{k}}
\|
\bm{C}
\|_{p,1}.
\end{split}
\end{equation}
Since the $\ell_{p,1}$ norm is separable with respect to each row, we can write the above problem as 
\begin{equation}
\begin{split}
\label{e6}
\min_{\bm{C}}
\sum_{i=1}^{M}
\frac{1}{2}
\|
(\bm{A}_{k+1})_{i*}+\rho_{k}^{-1}(\bm{\Delta}_{k})_{i*}
-
\bm{C}_{i*}
\|_{2}^{2}
+
\frac{\lambda}{\rho_{k}}
\|
\bm{C}_{i*}
\|_{p},
\end{split}
\end{equation}
where $\bm{F}_{i*}$ is the $i$th row of the matrix $\bm{F}$. Since this step is separable w.r.t. each row, we can employ parallel processing resources and reduce its computational time.


(3) Obtain the Lagrangian multipliers ($\bm{\Delta}_{k+1}$) while fixing ($\bm{C}_{k+1},\bm{A}_{k+1}$):
\begin{equation}
\begin{split}
\label{e10}
\bm{\Delta}_{k+1}
&
=
\bm{\Delta}_{k}
+
\rho_{k}
(\bm{C}_{k+1}-\bm{A}_{k+1})
.
\end{split}
\end{equation}

(5) Update the penalty parameter $\rho$ as $\rho_{k+1}=\mu\rho_{k}$, where $\mu>1$.


\section{Large Scale Subset Selection Via Row-Column Separation}



We can also restrict that $\text{diag}(\bm{A})=\bm{0}$ to avoid the samples to be self-represented. However, I want to mention that the proposed model solved by ADMM algorithm with three variables and does not have convergence results.

Then the model above can be 
\begin{equation}
\label{e2}
\min_{\bm{A}}
\|
\bm{X}
-
\bm{X}\bm{A}
\|_{F}^{2}
+
\lambda
\|
\bm{A}
\|_{p,1}
\quad
\text{s.t.}
\quad
\text{diag}(\bm{A})=\bm{0}.
\end{equation}
By introducing an auxiliary variable $\bm{C}$ into the optimization program, we can get
\begin{equation}
\begin{split}
\label{e3}
&
\min_{\bm{A},\bm{C}}
\|
\bm{X}
-
\bm{X}\bm{C}
\|_{F}^{2}
+
\lambda
\|
\bm{A}
\|_{p,1}
\\
& 
\text{s.t.}
\ 
\bm{C}=\bm{A}-\text{diag}(\bm{A}),
\end{split}
\end{equation}
whose solution for $\bm{A}$ coincides with the solution of Eq. (\ref{e3}). By introducing two Lagrangian multipliers $\bm{\Delta}$, the Lagrangian function of the Eq. (\ref{e3}) can be written as
\begin{equation}
\begin{split}
\label{e5}
&
\mathcal{L}
(\bm{A},\bm{C},\bm{\Delta},\rho)
=
\|
\bm{X}
-
\bm{X}\bm{C}
\|_{F}^{2}
+
\lambda
\|
\bm{A}
\|_{p,1}
\\
&
+
\langle
\bm{\Delta},
\bm{C}-(\bm{A}-\text{diag}(\bm{A}))
\rangle
+
\frac{\rho}{2}
\|
\bm{C}-(\bm{A}-\text{diag}(\bm{A})
\|_{F}^{2}
\end{split}
\end{equation}
Denote by ($\bm{C}_{k},\bm{A}_{k}$) the optimization variables at iteration $k$, by $\bm{\Delta}_{k}$ the Lagrangian multipliers at iteration $k$, and by $\rho_{k}$ the penalty parameter at iteration $k$. Taking detivatives of $\mathcal{L}$ with respect to the variables and setting the derivatives to be zeros, we can alternatively update the variables as follows:

(1) Obtain $\bm{A}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{A}$, while fixing ($\bm{C}_{k},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e8}
&
\bm{A}_{k+1}
=
\bm{J}-\text{diag}(\bm{J}),
\\
&
\bm{J}
=
\arg\min_{\bm{J}}
\frac{1}{2}
\|
\bm{C}_{k}
+
\rho_{k}^{-1}\bm{\Delta}_{k}
-
\bm{J}
\|_{F}^{2}
+
\frac{\lambda}{\rho_{k}}
\|
\bm{J}
\|_{p,1}.
\end{split}
\end{equation}

(2) Obtain $\bm{C}_{k+1}$ by minimizing $\mathcal{L}$ with respect to $\bm{C}$, while fixing ($\bm{A}_{k+1},\bm{\Delta}_{k}$). This is equivalent to solve the following problem:
\begin{equation}
\begin{split}
\label{e6}
\min_{\bm{C}}
\|
\bm{X}-\bm{X}\bm{C}
\|_{F}^{2}
+
\frac{\rho_{k}}{2}
\|
\bm{C}-\bm{A}_{k+1}+\frac{1}{\rho_{k}}\bm{\Delta}_{k}
\|_{F}^{2}
\end{split}
\end{equation}
This is a least squares regression problem which has a closed-form solution as 
\begin{equation}
\begin{split}
\label{e7}
\bm{C}_{k+1} 
&
=
(\bm{X}^{\top}\bm{X}+\frac{\rho_{k}}{2}\bm{I})^{-1}
(\bm{X}^{\top}\bm{X}+\frac{\rho_{k}}{2}\bm{A}_{k+1}-\frac{1}{2}\bm{\Delta}_{k})
.
\end{split}
\end{equation}

(3) Obtain the Lagrangian multipliers ($\bm{\Delta}_{k+1}$) while fixing ($\bm{C}_{k+1},\bm{A}_{k+1}$):
\begin{equation}
\begin{split}
\label{e10}
\bm{\Delta}_{k+1}
&
=
\bm{\Delta}_{k}
+
\rho_{k}
(\bm{C}_{k+1}-\bm{A}_{k+1})
.
\end{split}
\end{equation}

(5) Update the penalty parameter $\rho$ as $\rho_{k+1}=\mu\rho_{k}$, where $\mu>1$.


{
\small
\bibliographystyle{unsrt}
\bibliography{egbib}
}

\end{document}
